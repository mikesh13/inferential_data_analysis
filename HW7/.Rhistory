# ZeroPref_DF[is.na(ZeroPref_DF)] <- 0
All_Zero_Decision_DF <- merge(ZeroDecisionDF, Zero_NonDecisionsDF3, by="UserID")
Demographics <- read.csv("./UpdateDemographics.csv") %>% filter(Completed==1)
## Questions and Models
## Regression between Average Delay and riskgamble, zerocost pref
RegDF1 <- merge(RiskGamble, ZeroDecisionDF2, by="UserID")
RegDF2 <- merge(RegDF1, User_AverageDelay, by="UserID")
RegDF3 <- merge(RegDF2, Demographics, by="UserID")
RegDF3$RiskTaking <- scale(RegDF3$RiskTaking)
RegDF3$ZeroPrefMeasure2 <- scale(RegDF3$ZeroPrefMeasure2)
RegDF3$AveDelay <- scale(RegDF3$AveDelay)
RegDF3$Gender <- as.factor(RegDF3$Gender)
RegDF3$Age <- as.factor(RegDF3$Age)
colnames(RegDF3)[colnames(RegDF3)=="AveDelay"] <- "Average_Delay"
colnames(RegDF3)[colnames(RegDF3)=="ZeroPrefMeasure2"] <- "Preference_for_Zero_Cost"
colnames(RegDF3)[colnames(RegDF3)=="RiskTaking"]<- "Risk_Taking"
Fit1 <- lm(Average_Delay~Preference_for_Zero_Cost+Risk_Taking, data=RegDF3)
cowme <- Decisions1 %>%
mutate(UDD = Day*Decision) %>%
mutate(ALD = (AttackLoss)/100) %>%
group_by(UserID, Period) %>%
summarise(UpdateDecisionDay = sum(UDD), AttackLossDay = sum(ALD), Gambles=sum(Gamble), ZE = sum(UpdateCost==0)) %>%
mutate(DecisionW = ((11-UpdateDecisionDay)/10)) %>%
mutate(DecisionW = replace(DecisionW, which(DecisionW>1),0)) %>%
#mutate(Block = ifelse(Period < 8, 1, ifelse(Period<12, 2, ifelse(Period<16,3,4)))) %>%
mutate(Block = ifelse(Period < 6, 1, ifelse(Period<8, 2, ifelse(Period<10, 3, ifelse(Period<12,4,ifelse(Period<14,5,ifelse(Period<16,6,ifelse(Period<18,7,8)))))))) %>%
group_by(UserID, Block) %>%
summarise(GR=sum(DecisionW), AR = sum(AttackLossDay), Production=sum(Gambles), ZeroExperience = sum(ZE)) %>%
ungroup() %>%
group_by(UserID) %>%
mutate(AR1 = lag(AR)) %>%
mutate(Production1 = lag(Production))%>%
mutate(ZeroExperience1 = lag(ZeroExperience))
ggplot(cowme, aes(x=Block, y=GR, color=UserID)) +
geom_point()+
geom_line() +
theme(legend.position = "none") +
labs(x="Trial Block", y="Weighted Update Rate")
cowme1 <- cowme %>% dplyr::select(UserID, Block, GR) %>%
spread(Block, GR) %>%
rename(Block1 = `1`) %>%
rename(Block2 = `2`) %>%
rename(Block3 = `3`) %>%
rename(Block4 = `4`) %>%
rename(Block5 = `5`) %>%
rename(Block6 = `6`) %>%
rename(Block7 = `7`) %>%
rename(Block8 = `8`) %>%
mutate(B1B2 = Block2-Block1) %>%
mutate(B2B3 = Block3-Block2) %>%
mutate(B3B4 = Block4-Block3) %>%
mutate(B4B5 = Block5-Block4) %>%
mutate(B5B6 = Block6-Block5) %>%
mutate(B6B7 = Block7-Block6) %>%
mutate(B7B8 = Block8-Block7) %>%
gather(Block, RateofChange, B1B2:B7B8)
averateofchange <- cowme1 %>%
dplyr::select(UserID, Block, RateofChange) %>%
group_by(UserID) %>%
spread(Block, RateofChange) %>%
mutate(averate = sum(c(B1B2, B2B3, B3B4, B4B5, B5B6, B6B7, B7B8)))%>%
mutate(averate1 = averate/7)%>%
mutate(averate2 = mean(c(B1B2, B2B3, B3B4, B4B5, B5B6, B6B7, B7B8)))
RegDF4<-merge(RegDF3,averateofchange,by="UserID")
RegDF4$averate2 <- scale(RegDF4$averate2)
RegDF4$Age1 <- mapvalues(RegDF4$Age, from=c("18-25", "26-35", "36-45", "46-55","56-65"), to=c(1,2,3,4,5))
RegDF4$Age1 <- as.numeric(RegDF4$Age1)
Fit2 <- lm(Average_Delay~Preference_for_Zero_Cost+Risk_Taking, data=RegDF4)
RegDF5 <- merge(RegDF4, Payoffs1, by="UserID")
RegDF5$CurEndowment <- scale(RegDF5$CurEndowment)
cor.test(RegDF5$CurEndowment, RegDF5$Average_Delay)
cor.test(RegDF5$averate, RegDF5$Average_Delay)
cor.test(RegDF5$averate, RegDF5$CurEndowment)
#RegDF1$RiskAverse <- scale(RegDF1$RiskAverse)
RegDF1$DecisionPerformance <- scale(RegDF1$DecisionPerformance)
RegDF1$DecisionPerformance1 <- scale(RegDF1$DecisionPerformance1)
RegDF1$ZeroPrefMeasure2 <- scale(RegDF1$ZeroPrefMeasure2)
fit1 <- lm(DecisionPerformance~RiskAverse*ZeroPrefMeasure2, data=RegDF1)
fit1A <- lm(DecisionPerformance~RiskAverse+ZeroPrefMeasure1, data=RegDF1)
RegDF2$AveDelay <- scale(RegDF2$AveDelay)
fit1 <- lm(DecisionPerformance~RiskAverse+ZeroPrefMeasure2, data=RegDF2)
fit2 <- lm(AveDelay~RiskAverse+ZeroPrefMeasure2, data=RegDF2)
fit3 <- lm(DecisionPerformance1~RiskAverse+ZeroPrefMeasure2, data=RegDF2)
RegDF3 <- merge(RegDF2, Payoffs, by="UserID")
RegDF4 <- merge(RegDF3, HS_DF, by="UserID")
RegDF4[is.na(RegDF4)]<-0
RegDF4$HotStove3[is.infinite(RegDF4$HotStove3)]<-0
RegDF4$HotStove3 <- scale(RegDF4$HotStove3)
fit2A <- lm(Average_Delay~Risk_Aversion+Preference_for_Zero_Cost+HotStove3, data=RegDF4)
fit3 <- lm(TotalGamble~RiskAverse+ZeroPrefMeasure1, data=RegDF3)
ggplotRegression(lm(Average_Delay~Preference_for_Zero_Cost, data=RegDF2))
ggplotRegression(lm(Average_Delay~Risk_Aversion, data=RegDF2))
HSRegDF <- merge(ZeroDecisionDF2, WeightedHotStoveDecisionDF1, by="UserID")
HSRegDF1 <- merge(HSRegDF, RiskGamble, by="UserID")
HSRegDF2 <- merge(HSRegDF1, Payoffs1, by="UserID")
HSRegDF2$HotStove1 <- HSRegDF2$WeightedHotStove/HSRegDF2$AttackCount
HSRegDF2$HotStove <- scale(HSRegDF2$HotStove)
HSRegDF2$RiskAverse <- scale(HSRegDF2$RiskAverse)
HSRegDF2$ZeroPrefMeasure2 <- scale(HSRegDF2$ZeroPrefMeasure2)
HSRegDF2$CurEndowment <- scale(HSRegDF2$CurEndowment)
HSRegDF2$HotStove1 <- scale(HSRegDF2$HotStove1)
fit4 <- lm(HotStove1 ~ RiskAverse+ZeroPrefMeasure2 , data = HSRegDF2)
View(HSRegDF2)
## WHat is the relationship between predictors and final endowment
PayoffRegDF <- merge(User_AverageDelay, Payoffs1, by="UserID")
PayoffRegDF1 <- merge(PayoffRegDF, RiskGamble, by="UserID")
PayoffRegDF2 <- merge(PayoffRegDF1, ZeroDecisionDF2, by="UserID")
PayoffRegDF2$CurEndowment <- scale(PayoffRegDF2$CurEndowment)
PayoffRegDF2$AveDelay <- scale(PayoffRegDF2$AveDelay)
PayoffRegDF2$ZeroPrefMeasure2 <- scale(PayoffRegDF2$ZeroPrefMeasure2)
PayoffRegDF2$RiskAverse <- scale(PayoffRegDF2$RiskAverse)
fit5<-lm(CurEndowment ~ RiskAverse+AveDelay, data = PayoffRegDF2)
#RegDF <- merge(DecisionQualityDF, ZeroDecisionDF2, by="UserID")
## What is the relationship between risk aversion number of decisions
tey <- merge(RiskGamble, TotalNumberofDecisions, by="UserID")
cor.test(tey$RiskAverse,tey$TotalDecisions)
##Ans: the relationship is a positive significant one.. higher the risk aversion more are the number of update decisions
## cor = 0.24, p=0.014
testABC <- merge(ZeroPref_DF, DecisionQualityDF, by="UserID")
testABCD <- merge(RiskGamble, DecisionQualityDF, by="UserID")
testABCDE <- merge(Attack_CountDF, DecisionQualityDF, by="UserID")
cor.test(testABCD$RiskAverse, testABCD$DecisionPerformance)
plot(testABCD$RiskAverse, testABCD$DecisionPerformance)
## What is the relationship between risk aversion and hot stove effect
tey1 <- merge(RiskGamble, constemp, by="UserID")
## Nope nothing
## what is the relationship between risk aversion and payoff
D<-Decisions %>% filter(Period==20 & Day == 10) %>% dplyr::select(UserID,CurEndowment)
tey2 <- merge(RiskGamble, D, by="UserID")
## Nope nothing
## What is the relationship between risk aversion and delayed hot stove effect
tey3 <- merge(RiskGamble, DelayHotStoveDecisionDF, by="UserID")
## Nope nothing
## What is the relationship between risk aversion and period level hot stove effect
tey4 <- merge(RiskGamble, HotStoveDecisionDF, by="UserID")
ReactDecTemp <- merge(ReactiveDecisionDF, TotalNumberofDecisions, by="UserID")
ReactDecTemp$HotStove <- ReactDecTemp$ReactiveDecision/ReactDecTemp$TotalDecisions
tey5 <- merge(ReactDecTemp, RiskGamble, by="UserID")
## Note - I find that most of the participants who were not attacked updated every period. except for this participant A2551EKB87S2XY who updated only twice but was never attacked and was very lucky
## Count number of attacks in each period and count number of times attacked after update
Attack_After_Update <- Decisions1 %>% mutate(DecisionDay = Decision*Day, AttackLossDay = AttackLoss*Day)
Attack_After_Update$AttackDay <- Attack_After_Update$AttackLossDay/100
Attack_Count_Period <- Attack_After_Update %>% group_by(UserID,Period) %>% summarise(AttackCount = sum(AttackDay>0))
Decision_Day_Per_Period <- Attack_After_Update %>% group_by(UserID,Period) %>% summarise(DecisionDay1 = sum(DecisionDay)) %>% filter(DecisionDay1 != 0)
First_Attack_PerPeriod <- Attack_After_Update %>% group_by(UserID, Period) %>%  arrange(UserID, Period, Day) %>% filter(AttackDay!=0) %>% summarise(AttackDay1 = min(AttackDay)) %>% dplyr::select(UserID,Period,AttackDay1)
Update_After_Attack <- merge(First_Attack_PerPeriod, Decision_Day_Per_Period, by=c("UserID","Period"))
Update_After_Attack$test <- Update_After_Attack$AttackDay1 > Update_After_Attack$DecisionDay1
Attack_After_UpdateDF <- Update_After_Attack %>% group_by(UserID) %>% summarise(AAU = n())
## Measuring the relationship between attack after update and decision performance and total number of decisions
testABCDEF <- DecisionQualityDF %>% left_join(Attack_After_UpdateDF, by="UserID")
testABCDEF[is.na(testABCDEF)]<-0
testABCDEF$AAU <- as.factor(testABCDEF$AAU)
plot(testABCDEF$AAU, testABCDEF$DecisionPerformance)
fit <- aov(DecisionPerformance ~ AAU, testABCDEF)
summary.aov(fit)
plot(testABCDEF$AAU, testABCDEF$TotalDecisions)
fit <- aov(TotalDecisions~AAU, testABCDEF)
summary(fit)
## Measuring the relationship between low-cost availability to decision performance
testABC12 <- merge(DecisionQualityDF, Zero_Experience1, by="UserID")
## Measuring the number of attacks
#TempAttackCount <- Decisions1 %>% mutate(DecisionDay = Decision*Day, AttackLossDay = AttackLoss*Day)
#TempAttackCount$AttackDay <- TempAttackCount$AttackLossDay/100
#Attack_CountDF <- TempAttackCount %>% group_by(UserID) %>% summarise(AttackCount = sum(AttackDay>0))
Attack_CountDF <- Decisions1 %>%
group_by(UserID) %>%
summarise(AttackLoss1 = sum(AttackLoss))%>%
mutate(AttackCount = AttackLoss1/100)
test1 <- merge(User_AverageDelay, Attack_CountDF, by="UserID")
test2 <- merge(User_AverageDelay, ZeroDecisionDF1, by="UserID") ## higher the preference for zero cost higher the delay
test3 <- merge(User_AverageDelay, RiskGamble, by="UserID") ## higher delay lower the risk aversion
test4 <- merge(User_AverageDelay, Attack_UpdateDelay1, by="UserID")
test5 <- merge(User_AverageDelay, ReactiveDecisionDF, by="UserID")
test6 <- merge(User_AverageDelay, ConsHotStoveDecisionDF, by="UserID")
test7 <- merge(test6, TotalNumberofDecisions, by="UserID")
test8 <- merge(test6, ReactiveDecisionDF, by="UserID")
test8$HotStove <- test8$ConsTotalHotStove/test8$ReactiveDecision
test8[is.na(test8)]<-0
test9 <- merge(test8, TotalNumberofDecisions, by="UserID")
test10 <- merge(test9, Attack_CountDF, by="UserID")
test10$AttackCount1 <- test10$AttackCount+1
## Merging all the individual Data Frames
A1 <- full_join(User_AverageDelay,TotalNumberofDecisions)
A2 <- full_join(A1, HotStoveDecisionDF)
A2 <- full_join(A1, LiberalHotStoveDecisionDF)
A3 <- full_join(A2, ConsHotStoveDecisionDF)
Full_User_HotStove_DF <- full_join(A3, RiskGamble)
Full_User_HotStove_DF$PeriodHotStove <- Full_User_HotStove_DF$TotalHotStove/Full_User_HotStove_DF$TotalDecisions
test <- merge(Attack_UpdateDelay1, User_AverageDelay, by="UserID")
## Relationship between hot stove and the dependent variables
tempof <- merge(Attack_Response_DF, RiskGamble, by="UserID")
tempof1 <- merge(Attack_Response_DF, TotalNumberofDecisions, by="UserID")
tempof2 <- merge(Attack_Response_DF, User_AverageDelay, by="UserID")
tempof3 <- merge(Attack_Response_DF, DecisionQualityDF, by="UserID")
tempof4 <- merge(Attack_Response_DF, C, by="UserID")
teeey <- merge(ZeroDecisionDF1, Zero_Experience1, by="UserID")
teeey1 <- merge()
##Did everyone update on the following period after an attack
length(unique(Update_After_Attack$UserID))
## Ans: No, Of the 93 participants who faced atleast one attack, only 47 participants updated in the following period atleast once
## Pending ##
## How many always updated in the following period after an attack?
## Compare number of attacks with number of updates following an attack per participant
## Attack_Count_Period - summarise count of attacks per user
## Decision_Day_Per_Period without filtering
## Merge them
Attack_Count_Per_User <- Attack_After_Update %>% group_by(UserID) %>% summarise(AttackCount = sum(AttackDay>0))
Decision_Count_Per_User <- Attack_After_Update %>% group_by(UserID) %>% summarise(DecisionDay1 = sum(DecisionDay))
## Measuring the effect of factors on the learning to update - Good analysis but not relevant for paper
cowme2 <- cowme %>%
dplyr::select(UserID, Block, AR1, Production1, ZeroExperience1) %>%
filter(!is.na(AR1)) %>%
mutate(Block = factor(Block)) %>%
mutate(Block = revalue(Block, c("2"="B1B2", "3"="B2B3", "4"="B3B4"))) %>%
left_join(cowme1, by=c("UserID", "Block")) %>%
mutate(AR2 = cumsum(AR1)) %>%
mutate(Production2 = cumsum(Production1)) %>%
mutate(ZeroExperience2 = cumsum(ZeroExperience1))
cowme2$AR1 <- scale(cowme2$AR1)
cowme2$RateofChange <- scale(cowme2$RateofChange)
cowme2$Production1 <- scale(cowme2$Production1)
cowme2$Production2 <- scale(cowme2$Production2)
cowme2$AR2 <- scale(cowme2$AR2)
cowme2$ZeroExperience2 <- scale(cowme2$ZeroExperience2)
cowmefit <- lmer(RateofChange~ZeroExperience2 + AR2 + Production2 + (1|UserID), cowme2)
summary(cowmefit)
sjp.glmer(cowmefit, type="fe.slope")
ggplotRegression(lm(RateofChange~ZeroExperience2,cowme2))
averateofchange <- cowme1 %>%
dplyr::select(UserID, Block, RateofChange) %>%
group_by(UserID) %>%
spread(Block, RateofChange) %>%
mutate(averate = mean(c(B1B2, B2B3, B3B4)))
GR_DF <- merge(averateofchange, Payoffs1, by="UserID")
GR_DF1 <- merge(GR_DF, RiskGamble, by="UserID")
GR_DF2 <- merge(GR_DF1, ZeroDecisionDF2, by="UserID")
GR_DF2 <- merge(GR_DF2, User_AverageDelay, by="UserID")
GR_DF2 <- merge(GR_DF2, Attack_CountDF, by="UserID")
GR_DF2$CurEndowment <- scale(GR_DF2$CurEndowment)
GR_DF2$ZeroPrefMeasure2 <- scale(GR_DF2$ZeroPrefMeasure2)
GR_DF2$RiskAverse <- scale(GR_DF2$RiskAverse)
GR_DF2$AveDelay <- scale(GR_DF2$AveDelay)
GR_DF2[is.na(GR_DF2)]<-0
GR_DF2$averate <- scale(GR_DF2$averate)
GR_DF2 <- merge(GR_DF2, Demographics, by="UserID")
fit81<- lm(AveDelay ~ RiskAverse + ZeroPrefMeasure2 + averate + AttackLoss1, data = GR_DF2)
fit8<- lm(averate ~ AttackCount+RiskAverse+ZeroPrefMeasure2, data = GR_DF2)
##### Recreating the logit model of efrat to predict probability of early update fro
goatme <- Decisions1 %>%
group_by(UserID, Period) %>%
summarise(Attack = sum(AttackLoss), UpdateDecision = sum(Decision*Day)) %>%
mutate(Attack = Attack/100) %>%
mutate(Attack = replace(Attack, which(Attack>0),1)) %>%
mutate(EarlyUpdate = ifelse(UpdateDecision==1, 1, 0)) %>%
mutate(AttackinPrevPeriod = lag(Attack)) %>%
mutate(EarlyUpdateinPrevPeriod = lag(EarlyUpdate)) %>%
mutate(AttackinPrevPeriod = replace(AttackinPrevPeriod, is.na(AttackinPrevPeriod),0)) %>%
mutate(EarlyUpdateinPrevPeriod = replace(EarlyUpdateinPrevPeriod, is.na(EarlyUpdateinPrevPeriod),0))
goatme$EarlyUpdate <- as.factor(goatme$EarlyUpdate)
goatme$AttackinPrevPeriod <- as.factor(goatme$AttackinPrevPeriod)
goatme$EarlyUpdateinPrevPeriod <- as.factor(goatme$EarlyUpdateinPrevPeriod)
fit8 <- glmer(EarlyUpdate ~  Period + AttackinPrevPeriod + EarlyUpdateinPrevPeriod + EarlyUpdateinPrevPeriod:AttackinPrevPeriod + (1 | UserID), data = subset(goatme, Period > 4), family = binomial)
summary(fit8)
ella <- goatme %>% dplyr::select(UserID, EarlyUpdate, EarlyUpdateinPrevPeriod) %>%
mutate(EarlyUpdate = revalue(EarlyUpdate, c("0"="No", "1"="Yes"))) %>%
mutate(EarlyUpdateinPrevPeriod = revalue(EarlyUpdateinPrevPeriod, c("0"="No", "1"="Yes")))
ella1 <- goatme %>% dplyr::select(UserID, EarlyUpdate, AttackinPrevPeriod) %>%
mutate(EarlyUpdate = revalue(EarlyUpdate, c("0"="No", "1"="Yes"))) %>%
mutate(AttackinPrevPeriod = revalue(AttackinPrevPeriod, c("0"="No", "1"="Yes")))
tabl1 <- table(ella$EarlyUpdateinPrevPeriod,ella$EarlyUpdate)
mosaicplot(tabl1, main = "", xlab="Immediate Update in Previous Period",
ylab="Immediate Update")
tabl2 <- table(ella1$AttackinPrevPeriod, ella1$EarlyUpdate)
mosaicplot(tabl2, main = "",
xlab="Attack in Previous Period", ylab="Immediate Update")
Decisions1 %>%
mutate(UDD = Day*Decision) %>%
mutate(UDC = Decision*UpdateCost) %>%
mutate(ALD = AttackLoss/100) %>%
group_by(UserID, Period) %>%
summarise(UpdateDecisionDay = sum(UDD), UpdateDecisionCost=sum(UDC), AttackLoss1 = sum(ALD), RiskTaking1 = sum(Gamble==4|Gamble==0), Prod=sum(Gamble), ZeroCostDays1 = sum(UpdateCost==0)) %>%
mutate(WeightedDecision = ((11-UpdateDecisionDay)/10)) %>%
mutate(WeightedDecision = replace(WeightedDecision, which(WeightedDecision>1),0)) %>%
ungroup()%>%
group_by(Period)%>%
summarise(WeightedDecision1 = mean(WeightedDecision), SD=sd(WeightedDecision)) %>%
ggplot(aes(x=Period, y=WeightedDecision1))+
geom_smooth()+
ylim(0, 1) +
scale_x_continuous(breaks=c(4:20),labels=c(4:20))
Decisions1 %>%
mutate(UDD = Day*Decision) %>%
mutate(UDC = Decision*UpdateCost) %>%
mutate(ALD = AttackLoss/100) %>%
group_by(UserID, Period) %>%
summarise(UpdateDecisionDay = sum(UDD), UpdateDecisionCost=sum(UDC), AttackLoss1 = sum(ALD), RiskTaking1 = sum(Gamble==4|Gamble==0), Prod=sum(Gamble), ZeroCostDays1 = sum(UpdateCost==0)) %>%
mutate(WeightedDecision = ((11-UpdateDecisionDay)/10)) %>%
mutate(WeightedDecision = replace(WeightedDecision, which(WeightedDecision>1),0)) %>%
ungroup()%>%
group_by(Period)%>%
ggplot(aes(x=Period, y=WeightedDecision))+
geom_point()+
geom_jitter()+
geom_smooth()+
ylim(0, 1) +
scale_x_continuous(breaks=c(4:20),labels=c(4:20))
## some additional ones
cor.test(RegDF4$Risk_Aversion, RegDF4$Preference_for_Zero_Cost)
cor.test(RegDF4$TotalNonDecisions.x, RegDF4$Preference_for_Zero_Cost)
cor.test(RegDF4$Risk_Aversion, RegDF4$TotalNonDecisions.x)
cor.test(testA4$RiskAverse, testA4$AttackCount)
fit2A <- lm(Average_Delay~Risk_Aversion+Preference_for_Zero_Cost, data=RegDF4)
fit2A <- lm(Average_Delay~Risk_Aversion+HotStove3, data=RegDF4)
fit2A <- lm(Average_Delay~Risk_Aversion+Preference_for_Zero_Cost+HotStove3, data=RegDF4)
##Calculate proportion of reactive updates
libtempb <- Decisions1 %>%
group_by(UserID, Period) %>%
summarise(NumberofAttacks = sum(AttackLoss), UpdateDecision = sum(Decision))
libtempb$NumberofAttacks1 <- libtempb$NumberofAttacks/100
libtempb1 <- mutate(libtempb, hotstove = lag(NumberofAttacks1)*UpdateDecision)
libtempb1[is.na(libtempb1)] <- 0
libtempb2 <- libtempb1 %>% filter(Period==4) %>% mutate(hotstove = UpdateDecision+hotstove)
libtempb3 <- libtempb1 %>% left_join(libtempb2,by=c("UserID","Period"))
libtempb3[is.na(libtempb3)] <- 0
libtempb3$hotstove <- libtempb3$hotstove.x+libtempb3$hotstove.y
libtempb3 <- libtempb3 %>% mutate(hotstove = replace(hotstove,which(hotstove>1), 1))
ReactiveDecisionDF <- libtempb3 %>%
group_by(UserID) %>%
mutate(ReactiveDecision = sum(hotstove)) %>%
filter(Period==20) %>%
dplyr::select(UserID,ReactiveDecision) %>%
arrange(UserID)
Reactivetemp <- merge(ReactiveDecisionDF, TotalNumberofDecisions, by="UserID")
Reactivetemp$ReactDecision <- Reactivetemp$ReactiveDecision/Reactivetemp$TotalDecisions
##Calculate period level hot stove effect
tempb <- Decisions1 %>%
group_by(UserID, Period) %>%
summarise(NumberofAttacks = sum(AttackLoss), UpdateDecision = sum(Decision))
tempb$NumberofAttacks1 <- tempb$NumberofAttacks/100
tempb1 <- mutate(tempb, hotstove = lag(NumberofAttacks1)*UpdateDecision)
tempb1[is.na(tempb1)] <- 0
tempb3 <- tempb1
tempb3 <- tempb3 %>% mutate(hotstove = replace(hotstove,which(hotstove>1), 1))
HotStoveDecisionDF <- tempb3 %>% group_by(UserID) %>% mutate(TotalHotStove = sum(hotstove)) %>% filter(Period==20) %>% dplyr::select(UserID,TotalHotStove)
#tempb2 <- tempb1 %>% filter(Period==4) %>% mutate(hotstove = UpdateDecision+hotstove)
#tempb3 <- tempb1 %>% left_join(tempb2,by=c("UserID","Period"))
#tempb3[is.na(tempb3)] <- 0
#tempb3$hotstove <- tempb3$hotstove.x+tempb3$hotstove.y
## Measuring the attack - update delay
Attack_UpdateDelay <- AUTemp1 %>%  filter(AttackDay2>0, DecisionDay1>0) %>%
mutate(AUDelay = ((10-AttackDay2)+DecisionDay1-1)) %>%
mutate(UDelay = DecisionDay1-1)
Attack_UpdateDelay1 <- Attack_UpdateDelay %>%
group_by(UserID) %>%
summarise(Sensitivity = mean(AUDelay), Distance = mean(UDelay)) %>%
arrange(UserID)
## Updated to above - attack update delay for all participants with no decision considered a 11
AUTemp1$DecisionDay1[AUTemp1$DecisionDay1==0]<-11
Attack_UpdateDelay <- AUTemp1 %>%  filter(AttackDay2>0) %>%
mutate(AUDelay = ((10-AttackDay2)+DecisionDay1-1)) %>%
mutate(UDelay = DecisionDay1-1)
Attack_UpdateDelay1 <- Attack_UpdateDelay %>%
group_by(UserID) %>%
summarise(Sensitivity = mean(AUDelay), Distance = mean(UDelay), MedDistance = median(UDelay)) %>%
arrange(UserID)
Attack_UpdateDelay2 <- TotalNumberofDecisions %>% left_join(Attack_UpdateDelay1,by=c("UserID"))
# Attack_UpdateDelay1 <- Attack_UpdateDelay %>% group_by(UserID) %>% summarise(Sensitivity = mean(AUDelay))
# Attack_UpdateDelay2 <- Attack_UpdateDelay %>% group_by(UserID) %>% summarise(Distance = mean(UDelay))
# AUTemp[is.na(AUTemp)] <- 0
#
# Attack_UpdateDelay <- AUTemp %>%
#   group_by(UserID, Period) %>%
#   arrange(UserID, Period, Day) %>%
#   filter(AttackDay1!=0) %>%
#   summarise(AttackDay2 = max(AttackDay1), DecisionDay2 = sum(DecisionDay)) %>%
#   filter(AttackDay2>0, DecisionDay2>0)
#
# Attack_UpdateDelay <- AUTemp %>% group_by(UserID,Period) %>% summarise(PAttackDay = sum(Att)) filter(AttackDay>0, DecisionDay>0)
##### Old Analysis ########
Temp <- Decisions %>% mutate(DecisionDay = Decision*Day) %>% group_by(UserID,Period) %>% summarise(DD = sum(DecisionDay))
Temp$DD[Temp$DD==0]<-11 ##Setting no updates as 11 to indicate highest delay
Temp$Period <- as.factor(Temp$Period)
Temp1 <- Temp %>% group_by(Period) %>% summarise(MeanDD = mean(DD), MedianDD = median(DD))
a<- Temp %>% filter(DD!=11) #Filter out only updates
a %>% arrange(Period) %>%
ggplot(aes(x=Period,y=DD)) +
geom_boxplot()
b<- Temp %>% filter(Period!=3, DD==11) %>% group_by(Period) %>% tally() #Filter in only no-updates
b$NoUpdatePercent <- b$n/97*100
b$UpdatePercent <- (97-b$n)/97*100
## Comparing total gamble with final rewards
C<-Decisions %>% group_by(UserID) %>% summarise(TotalGamble = sum(Gamble))
D<-Decisions %>% filter(Period==20 & Day == 10) %>% dplyr::select(UserID,CurEndowment)
E<-merge(C,D,by="UserID")
## Calculate total number of update decisions
tempa <- Decisions %>% group_by(UserID) %>% mutate(TotalDecisions = sum(Decision))
tempb <- Decisions %>%
group_by(UserID, Period) %>%
summarise(NumberofAttacks = sum(AttackLoss), UpdateDecision = sum(Decision))
tempb$NumberofAttacks1 <- tempb$NumberofAttacks/100
tempb1 <- mutate(tempb, hotstove = lag(NumberofAttacks1)*UpdateDecision)
tempb2 <- tempb1 %>% filter(Period!=3) %>% filter(Period==4)%>%mutate(hotstove = UpdateDecision+hotstove)
tempb3 <- tempb1 %>% filter(Period!=3) %>% left_join(tempb2,by=c("UserID","Period"))
tempb3[is.na(tempb3)] <- 0
tempb3$hotstove <- tempb3$hotstove.x+tempb3$hotstove.y
## all updates following an update
tempc1 <-  Decisions %>%
group_by(UserID) %>%
mutate(TotalDecisions = sum(Decision)) %>% filter(Period==20, Day==10) %>% dplyr::select(UserID,TotalDecisions)
##updates on day 1 following an update
tempd <-  Decisions %>%
group_by(UserID) %>%
filter(Day==1) %>%
mutate(TotalDecisions1 = sum(Decision)) %>% filter(Period==20) %>% dplyr::select(UserID,TotalDecisions1)
tempd <- tempc %>% filter(Period==20, Day==10) %>% dplyr::select(UserID,TotalDecisions)
tempb3 <- tempb3 %>% mutate(hotstove = replace(hotstove,which(hotstove>1), 1))
tempb4 <- tempb3 %>% group_by(UserID) %>% mutate(TotalHotStove = sum(hotstove)) %>% filter(Period==20) %>% dplyr::select(UserID,TotalHotStove)
hotstovemeasure <- merge(tempd,tempb4,by="UserID")
hotstovemeasure$hotstove <- hotstovemeasure$TotalHotStove/hotstovemeasure$TotalDecisions
Temp_dat <- Temp %>% group_by(UserID) %>% summarise(AveDelay=mean(DD))
hotstovemeasure1 <- merge(hotstovemeasure,Temp_dat, by="UserID")
cor.test(hotstovemeasure1$hotstove,hotstovemeasure1$AveDelay)
RiskGamble <-Decisions %>% group_by(UserID) %>% summarise(RiskAverse = sum(Gamble == 2))
hotstovemeasure2 = merge(hotstovemeasure1,RiskGamble,by="UserID")
cor.test(hotstovemeasure2$RiskAverse,hotstovemeasure2$hotstove)
cor.test(hotstovemeasure2$RiskAverse,hotstovemeasure2$AveDelay)
cor.test(hotstovemeasure1$hotstove,hotstovemeasure1$AveDelay)
##count number of zero cost decisions
Decisions$ZeroUpdate <- Decisions$Decision + Decisions$UpdateCost
Decisions <- Decisions %>% mutate(ZeroUpdate = replace(ZeroUpdate,which(ZeroUpdate>1), 0))
ZeroDecisionDF <- Decisions %>% filter(Period!=3) %>% group_by(UserID) %>% summarise(TotalZeroUpdate = sum(ZeroUpdate))
FullDF2 <- merge(hotstovemeasure2, ZeroDecisionDF, by="UserID")
FullDF2$ZeroUpdateProp <- FullDF2$TotalZeroUpdate/FullDF2$TotalDecisions
cor.test(FullDF2$ZeroUpdateProp,FullDF2$AveDelay)
cor.test(FullDF2$ZeroUpdateProp,FullDF2$RiskAverse)
#sheep$UpdateDecisionCost<-as.factor(sheep$UpdateDecisionCost)
#sheep1 <- sheep %>%
# mutate(WeightedDecision = ifelse(WeightedDecision<1,0,1))
#sheep2 <- sheep %>%
#  filter(WeightedDecision!=0)
#%>%
# mutate(WeightedDecision = replace(WeightedDecision, which(WeightedDecision==1),0))%>%
#  mutate(WeightedDecision = replace(WeightedDecision, which(WeightedDecision>0),1))
#sheep3 <- sheep %>%
#  mutate(WeightedDecision = 1-WeightedDecision) %>%
#  mutate(WeightedDecision = replace(WeightedDecision, which(WeightedDecision<1),0))
#sheep1[,-c(1,4,3,5,9,10)]<-scale(sheep1[,-c(1,4,3,5,9,10)])
#sheep2[,-c(1,4,5,9,10)]<-scale(sheep2[,-c(1,4,5,9,10)])
#sheep3[,-c(1,4,3,5,9,10)]<-scale(sheep3[,-c(1,4,3,5,9,10)])
#sheep1$WeightedDecision <- as.factor(sheep1$WeightedDecision)
#sheep3$WeightedDecision <- as.factor(sheep3$WeightedDecision)
#sheep2$WeightedDecision <- as.factor(sheep2$WeightedDecision)
## WeightedDecision ~ Period Number + cost of update + Number of zero cost days in the previous period + AttacksinPrevPeriod + RiskTakinginPrevPeriod + production in prev period + Update decisions in previous period
# sheepfit1 <- glmer(WeightedDecision ~ Period+AttackRate+RiskTaking+ZeroCostDays+PrevImmediateUpdate+PrevNoUpdate+AttackRate:PrevImmediateUpdate + (1|UserID), data=sheep1, family = binomial)
# sheepfit2 <- lmer(WeightedDecision ~ Period+UpdateDecisionCost +AttackRate+RiskTaking+ZeroCostDays+PrevImmediateUpdate+PrevNoUpdate + (1|UserID), data=sheep2)
# sheepfit3 <- glmer(WeightedDecision ~ Period+AttackRate+RiskTaking+ZeroCostDays+PrevImmediateUpdate+PrevNoUpdate +(1|UserID), data=sheep3, family = binomial)
#
#
# shapfit <- polr(WeightedDecision ~ AttackRate+RiskTaking+ZeroCostDays+PrevImmediateUpdate+PrevNoUpdate+AttackRate*PrevImmediateUpdate, data=shap)
# (ctable <- coef(summary(shapfit)))
# p <- pnorm(abs(ctable[, "t value"]), lower.tail = FALSE) * 2
# (ctable <- cbind(ctable, "p value" = p))
#
# library(MCMCglmm)
# prior <- list (B=list(mu=rep(0,3), V=gelman.prior(~ Period+UpdateDecisionCost +AttackRate+RiskTaking+ZeroCostDays+PrevImmediateUpdate+PrevNoUpdate, data=shap, scale=1+pi^2/3)),
#                R = list(V = 1, fix= 1),
#                G = list(G1= list(V=1e-6, nu=-1)))
# ERiskModel <- MCMCglmm(WeightedDecision ~ Period+UpdateDecisionCost +AttackRate+RiskTaking+ZeroCostDays+PrevImmediateUpdate+PrevNoUpdate,
#                        data=shap, random=~UserID,family = "ordinal", prior= prior, nitt=600000, burnin=100000, thin=500)
ggplot(data = survey, aes(x = abs(survey$UberMoney-mean(survey$UberMoney)))) +
geom_histogram(bins = 10, fill = "#b7a57a")
library(tidyverse)
library(xtable)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(xtable)
setwd("~/Documents/UW/2020Winter/INDE546_InferentialDataAnalysis/HW/HW7")
survey <- read.csv("Class_Survey_W20.csv", header = TRUE)
survey <- survey %>%
rename(UberMoney = In.the.past.7.days..how.much.did.you.spend.on.Uber.Lyft.or.other.ride.hailing.apps..in.US.dollars..) %>%
mutate(UberMoney = replace_na(UberMoney, 0)) %>%
rename(UberRide = In.the.past.7.days..how.many.rides.did.you.hail.using.an.Uber.Lyft.app..) %>%
mutate(UberRide = replace_na(UberRide, 0))
survey <- survey %>%
rename(UberMoney = In.the.past.7.days..how.much.did.you.spend.on.Uber.Lyft.or.other.ride.hailing.apps..in.US.dollars..) %>%
mutate(UberMoney = replace_na(UberMoney, 0)) %>%
rename(UberRide = In.the.past.7.days..how.many.rides.did.you.hail.using.an.Uber.Lyft.app..) %>%
mutate(UberRide = replace_na(UberRide, 0))
setwd("~/Documents/UW/2020Winter/INDE546_InferentialDataAnalysis/HW/HW7")
survey <- read.csv("Class_Survey_W20.csv", header = TRUE)
survey <- survey %>%
rename(UberMoney = In.the.past.7.days..how.much.did.you.spend.on.Uber.Lyft.or.other.ride.hailing.apps..in.US.dollars..) %>%
mutate(UberMoney = replace_na(UberMoney, 0)) %>%
rename(UberRide = In.the.past.7.days..how.many.rides.did.you.hail.using.an.Uber.Lyft.app..) %>%
mutate(UberRide = replace_na(UberRide, 0))
survey <- survey %>%
rename(UberMoney = In.the.past.7.days..how.much.did.you.spend.on.Uber.Lyft.or.other.ride.hailing.apps..in.US.dollars..) %>%
mutate(UberMoney = replace_na(UberMoney, 0)) %>%
rename(UberRide = In.the.past.7.days..how.many.rides.did.you.hail.using.an.Uber.Lyft.app..) %>%
mutate(UberRide = replace_na(UberRide, 0))
survey <- survey %>%
rename(UberMoney = In.the.past.7.days..how.much.did.you.spend.on.Uber.Lyft.or.other.ride.hailing.apps..in.US.dollars..) %>%
mutate(UberMoney = replace_na(UberMoney, 0)) #%>%
survey <- survey %>% rename(UberMoney = In.the.past.7.days..how.much.did.you.spend.on.Uber.Lyft.or.other.ride.hailing.apps..in.US.dollars..)
setwd
setwd("~/Documents/UW/2020Winter/INDE546_InferentialDataAnalysis/HW/HW7")
setwd("~/Documents/UW/2020Winter/INDE546_InferentialDataAnalysis/HW/HW7")
survey <- read.csv("Class_Survey_W20.csv", header = TRUE)
survey <- survey %>% rename(UberMoney = In.the.past.7.days..how.much.did.you.spend.on.Uber.Lyft.or.other.ride.hailing.apps..in.US.dollars..)
library(tidyverse)
library(xtable)
survey <- survey %>% rename(UberMoney = In.the.past.7.days..how.much.did.you.spend.on.Uber.Lyft.or.other.ride.hailing.apps..in.US.dollars..)
