---
title: "LB_Project"
author: "Michael Shieh"
date: "3/14/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
setwd("~/Documents/UW/2020Winter/INDE546_InferentialDataAnalysis/Project/Dataset/")
library(tidyverse)
data <- read.csv("FinalDataset.csv", header = TRUE)
```

```{r}
data <- read.csv("FinalDataset.csv", header = TRUE)
```

```{r}
data <- data %>% filter(R.D.Expenses > 0) %>% drop_na(R.D.Expense.Growth)
```

```{r}
test <- lm(R.D.Expenses ~ Revenue + Revenue.Growth + Total.shareholders.equity + returnOnEquity + returnOnAssets + Net.Income, data = data)


test <- lm(R.D.Expense.Growth ~ Revenue  + returnOnEquity, data = data)
summary(test)
```
```{r}
for(name in colnames(data)){
  if (grepl("equity", name, ignore.case = TRUE) == TRUE){
    print(name)
  }
}
```

```{r}
library(glmnet)
data_final <- read.csv("Dataset/FinalDataset.csv", header = TRUE)
data_final <- data_final[,c(-1,-222,-223, -224)]
data_final <- data_final %>% drop_na(RD.OI.Ratio)

train.ix <- sample(nrow(data_final),floor( nrow(data_final)) * 4 / 5 )
data.train <- data_final[train.ix,]
# Create a testing data (half the original data size)
data.test <- data_final[-train.ix,]

trainX <- as.matrix(data.train[,-220]) # Here, I did more lines of code for data preprocessing. This is because of the data format requirement by the package "glmnet"
testX <- as.matrix(data.test[,-220])
trainY <- as.matrix(data.train[,220])
testY <- as.matrix(data.test[,220])
```


```{r}
#fit = glmnet(trainX,trainY, family=c("gaussian"))
fit = glmnet(as.matrix(data_final[,-220]), as.matrix(data_final[,220]), family=c("gaussian"))
cv.fit = cv.glmnet(as.matrix(data_final[,-220]), as.matrix(data_final[,220]))
plot(cv.fit)
plot(fit,label = TRUE)
plot(fit, xvar="dev", label=TRUE)
```

```{r}
# Step 6 -> To view the selected best model and the corresponding coefficients
cv.fit$lambda.min # cv.fit$lambda.min is the best lambda value that results in the best model with smallest mean-squared error
coef(cv.fit, s = "lambda.min") # This extracts the fitted regression parameters of the linear regression model using this lambda value. See how sparse it is. 
y_hat <- predict(cv.fit, newx = testX, s = "lambda.min") # This is to predict using the best model selected by LASSO
cor(y_hat, data.test$MMSCORE) #For regression model, you can use correlation to measure how close your predictions with the true outcome values of the data points 
mse <- mean((y_hat - data.test$MMSCORE)^2) # Another metric is the mean squared error (mse)
mse
```

```{r}
# Step 7 -> Re-fit the regression model with selected variables by LASSO
# As LASSO put l1-norm penalty on the regression parameters, even the significant variables are selected, their regression parameters are biased towards smaller values. Thus, there is a suggestion to re-fit the regression model with selected variables by LASSO. Then you will get unbiased estimates of the regression parameters and R-squareds, p-values. 
var_idx <- which(coef(cv.fit, s = "lambda.min") != 0)
#lm.reduced <- lm(RD.OI.Ratio ~ ., data = data_final[,c(var_idx, 220)])

lm.reduced.test <- lm(RD.OI.Ratio.ST ~ ., data = data_final[,c(var_idx, 221)])
summary(lm.reduced.test) # compare the least-squares estimates of the regression parameters with the regression parameters from LASSO
```

```{r}
lm.test2 <- lm(RD.OI.Ratio ~ ., data =  data_final[,c(1,6,7,8,28,36,37,38,105,220)])
lm.test3 <- lm(RD.OI.Ratio ~ ., data =  data_final[,c(1,6,7,36,37,38,105,220)])
summary(lm.test3)
```

```{r}
confint(lm.test3)
table <- cbind(summary(lm.test3)$coefficient, confint(lm.test3))
print(table)
par(mfrow = c(2, 2))
plot(lm.test3)
```

```{r}
lm.test3.reduced <- step(lm.test3, direction = "backward", test = "F")
summary(lm.test3.reduced)
```

```{r}
lm.existing <- lm(RD.OI.Ratio ~ Revenue + Revenue.Growth + Total.shareholders.equity + returnOnEquity + returnOnAssets + Net.Income, data = data_final)

summary(lm.existing)
```


```{r}

```


```{r}
lm.fun <- lm(RD.OI.Ratio ~., data = data_final)
lm.fun.reduced <- step(lm.fun, steps = "backward", test = "F")

summary(lm.fun)
```